{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FdngmQE2960N"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import warnings\n",
    "import datetime\n",
    "import math\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np \n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import src.data_utils as d\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 999)\n",
    "pd.set_option('display.max_rows', 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FCoYF5AF960c"
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If some files don't exist you should generate them via `notebooks/additional_data_preprocessing.ipynb`.\n",
    "\n",
    "road_segments_path = '../data/road_segments_processed/road_segments_vds_uber.shp'\n",
    "sample_submission_path = '../data/SampleSubmission.csv'\n",
    "train_cleaned_path = '../data/train_cleaned.csv'\n",
    "result_weather_path = '../data/weather_processed/weather.csv'\n",
    "cameras_count_path = '../data/SANRAL_processed/cameras_counts.csv'\n",
    "vds_locations_path = '../data/SANRAL_processed/vds_locations.csv'\n",
    "result_hourly_path = '../data/SANRAL_processed/VDS_hourly_all.csv'\n",
    "cameras_comm_dates_path = '../data/SANRAL_processed/cameras_comnissionning_dates.csv'\n",
    "result_segment_ttime_path = '../data/uber_processed/segment_ttime_daily.csv'\n",
    "sid_neigh_path = '../data/uber_processed/sid_neighbors.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QJmEiQwK960e"
   },
   "outputs": [],
   "source": [
    "road_segments        = gpd.read_file(road_segments_path)\n",
    "\n",
    "sid_neighbors        = json.load(open(), 'r')\n",
    "\n",
    "sample_sub           = pd.read_csv(sample_submission_path)\n",
    "train_raw            = pd.read_csv(train_cleaned_path, parse_dates=['Occurrence Local Date Time'])\n",
    "weather              = pd.read_csv(result_weather_path, parse_dates=['datetime'])\n",
    "road_segment_cam_cnt = pd.read_csv(cameras_count_path, sep=';')\n",
    "VDS_locations        = pd.read_csv(vds_locations_path) \n",
    "VDS_hourly           = pd.read_csv(result_hourly_path, parse_dates=['Date of Collection Period'])\n",
    "commission_dates     = pd.read_csv(cameras_comm_dates_path)\n",
    "segment_ttime_daily  = pd.read_csv(result_segment_ttime_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJkdlECD960h"
   },
   "source": [
    "## Preparing train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "SN-cF80V960i",
    "outputId": "9c2e0c45-c1ce-4614-935f-8fd3d21e0a3d"
   },
   "outputs": [],
   "source": [
    "train_raw['longitude'] = pd.to_numeric(train_raw['longitude'], errors='coerce')\n",
    "train_raw.dropna(subset=['longitude'], inplace=True)\n",
    "train_raw['longitude'] = train_raw['longitude'].astype('float')\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "GHZnxyi3960q",
    "outputId": "5e55a151-67e3-4055-d178-50c0bea2bee5"
   },
   "outputs": [],
   "source": [
    "sids = train_raw['road_segment_id'].unique()\n",
    "\n",
    "dts  = pd.date_range('2016-01-01',\n",
    "                     '2019-01-01 00:00:00',\n",
    "                     freq=\"1h\")\n",
    "\n",
    "tr   = pd.DataFrame({'datetime':dts})\n",
    "\n",
    "for sid in sids:\n",
    "    tr[str(sid)] = 0\n",
    "    events = train_raw.loc[train_raw['road_segment_id'] == sid]\n",
    "    dts = events['Occurrence Local Date Time'].dt.floor('H')\n",
    "    dates = dts.astype(str).unique()\n",
    "    tr.loc[tr['datetime'].isin(dates), sid] = 1\n",
    "tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "ZDxgNeTP960u",
    "outputId": "41bb341c-5f3b-42ec-bc94-baa2d80de98e"
   },
   "outputs": [],
   "source": [
    "train = pd.DataFrame({\n",
    "    'datetime x segment_id':np.concatenate([[str(x) + \" x \" + str(c) \n",
    "                                             for c in sids] \n",
    "                                            for x in tr['datetime']]),\n",
    "    'datetime':np.concatenate([[str(x) for c in sids] for x in tr['datetime']]),\n",
    "    'segment_id':np.concatenate([[str(c) for c in sids] for x in tr['datetime']]),\n",
    "    'y':tr[sids].values.flatten()\n",
    "})\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tGvJwJbK960x"
   },
   "source": [
    "## Preparing Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "L5fdaQYD960y",
    "outputId": "e14b7655-3c1b-4cf6-e4ef-d539b4a77e7e"
   },
   "outputs": [],
   "source": [
    "sample_sub[['datetime','segment_id']] = sample_sub['datetime x segment_id'].str.split(' x ',expand=True)\n",
    "test = sample_sub[['datetime x segment_id', 'datetime', 'segment_id', 'prediction']]\n",
    "test = test.rename(columns={'prediction': 'y'})\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BYC0WToK9603"
   },
   "source": [
    "## Road Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "-qE7LmMH9604",
    "outputId": "57f4733a-9433-47df-eed1-7bc752533ac6"
   },
   "outputs": [],
   "source": [
    "road_segments['LANES'].replace(0,1,inplace=True)\n",
    "road_segments['lane_width']   = road_segments['WIDTH'] / road_segments['LANES']\n",
    "road_segments['sinuosity']    = road_segments.geometry.apply(u.get_sinuosity) - 1\n",
    "road_segments['lon_centroid'] = road_segments.geometry.apply(lambda s: s.centroid.coords[0][0])\n",
    "road_segments['lat_centroid'] = road_segments.geometry.apply(lambda s: s.centroid.coords[0][1])\n",
    "road_segments['dist_to_center'] = np.sqrt((road_segments.lon_centroid - 18.421148) ** 2 +\n",
    "                                          (road_segments.lon_centroid + 33.920460) ** 2)\n",
    "road_segments['orientation']  = road_segments['geometry'].apply(u.get_orientation)\n",
    "road_segments.drop(['REGION','geometry'],axis=1,inplace=True)\n",
    "\n",
    "#Adding camera count on each segment\n",
    "road_segments = road_segments.merge(road_segment_cam_cnt, left_on='segment_id', right_on='segment_id', how='left')\n",
    "road_segments[['camera_count','vms_count','vds_count']] = road_segments[['camera_count','vms_count','vds_count']].fillna(0)\n",
    "road_segments['vds_id'] = road_segments['vds_id'].replace({'VDS139IO':'VDS139', \n",
    "                                                            'VDS912IO':'VDS912',\n",
    "                                                            'VDS913IO':'VDS913',\n",
    "                                                            'VDS914IO':'VDS914'}, regex=True)\n",
    "road_segments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_segments['num'] = road_segments['num'].astype(str)\n",
    "road_segments['neighbors_to_center']   = road_segments['num'].astype(str)\n",
    "road_segments['neighbors_from_center'] = road_segments['num'].astype(str)\n",
    "\n",
    "for direction in sid_neighbors:\n",
    "    for sid, neighbor in sid_neighbors[direction].items():\n",
    "        neighbor = ','.join(str(x) for x in neighbor)\n",
    "        road_segments['neighbors_{0}'.format(direction)].replace(sid, neighbor, inplace=True)\n",
    "        \n",
    "road_segments['neighbors_to_center'][road_segments['neighbors_to_center'] == road_segments['num']] = 'None'\n",
    "road_segments['neighbors_from_center'][road_segments['neighbors_from_center'] == road_segments['num']] = 'None'\n",
    "\n",
    "road_segments['neighbors'] = road_segments['neighbors_to_center'] + ',' + road_segments['neighbors_from_center']\n",
    "\n",
    "is_not_none = lambda x: x != 'None'\n",
    "road_segments['neighbors'] = road_segments['neighbors'].apply(lambda s: \n",
    "                                                              set(filter(is_not_none, list(str(s).split(',')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_neighbors = dict(zip(road_segments.num, road_segments['neighbors']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_segments.drop(['neighbors', 'neighbors_to_center', 'neighbors_from_center'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "road_segments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5034Ltxmv5e"
   },
   "source": [
    "## VDS_data\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-sK0hz6pBJmU"
   },
   "outputs": [],
   "source": [
    "VDS_hourly = u.clean_vds_data(VDS_hourly)\n",
    "\n",
    "\n",
    "dts = pd.date_range('2016-01-01', '2019-03-31 23:00:00', freq=\"1h\")\n",
    "tr = pd.DataFrame({'datetime':dts})\n",
    "VDS_list = VDS_hourly.vds_id.unique()\n",
    "VDS_hours = pd.DataFrame({\n",
    "    'datetime': np.concatenate([[str(x) for c in VDS_list] for x in tr['datetime']]),\n",
    "    'vds_id': np.concatenate([[str(c) for c in VDS_list] for x in tr['datetime']])}\n",
    ")\n",
    "VDS_hours['datetime'] = VDS_hours['datetime'].astype('datetime64[ns]')\n",
    "VDS_hourly = VDS_hours.merge(VDS_hourly, how='left', on=['datetime', 'vds_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B_WRu3a-tSTb"
   },
   "outputs": [],
   "source": [
    "VDS_hourly['hour']    = VDS_hourly.datetime.dt.hour\n",
    "VDS_hourly['weekday'] = VDS_hourly.datetime.dt.weekday\n",
    "VDS_hourly['year']    = VDS_hourly.datetime.dt.year\n",
    "VDS_hourly['month']   = VDS_hourly.datetime.dt.month\n",
    "VDS_hourly['quarter'] = (VDS_hourly.datetime.dt.month - 1) // 3 + 1\n",
    "\n",
    "VDS_hourly['traffic_unknown'] = VDS_hourly['traffic_total'].isnull().astype(int)\n",
    "VDS_hourly['speed_unknown']   = VDS_hourly['avg_speed'].isnull().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lNEUNy39Os1N"
   },
   "outputs": [],
   "source": [
    "cols_fill_na = ['traffic1', 'traffic2', 'traffic3', 'traffic_total', 'avg_speed']\n",
    "for f in cols_fill_na:\n",
    "    VDS_hourly[f] = VDS_hourly[f].fillna(VDS_hourly.groupby(['vds_id', 'year', 'month', 'weekday', 'hour'])[f].transform('mean'))\n",
    "    VDS_hourly[f] = VDS_hourly[f].fillna(VDS_hourly.groupby(['vds_id', 'month', 'weekday', 'hour'])[f].transform('mean'))\n",
    "    VDS_hourly[f] = VDS_hourly[f].fillna(VDS_hourly.groupby(['vds_id', 'weekday', 'hour'])[f].transform('mean'))\n",
    "    VDS_hourly[f] = VDS_hourly[f].fillna(VDS_hourly.groupby(['vds_id', 'hour'])[f].transform('mean'))\n",
    "    VDS_hourly[f] = VDS_hourly[f].fillna(VDS_hourly.groupby(['vds_id', 'weekday'])[f].transform('mean'))\n",
    "    VDS_hourly[f] = VDS_hourly[f].fillna(VDS_hourly.groupby(['vds_id'])[f].transform('mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__Bh6yL3uVfV"
   },
   "outputs": [],
   "source": [
    "VDS_hourly['mean_avg_speed'] = VDS_hourly.groupby(['vds_id', 'weekday', 'hour'])['avg_speed'].transform('mean')\n",
    "VDS_hourly['std_avg_speed']  = VDS_hourly.groupby(['vds_id', 'weekday', 'hour'])['avg_speed'].transform('std')\n",
    "\n",
    "VDS_hourly['mean_traffic']   = VDS_hourly.groupby(['vds_id', 'weekday', 'hour'])['traffic_total'].transform('mean')\n",
    "VDS_hourly['std_traffic']    = VDS_hourly.groupby(['vds_id', 'weekday', 'hour'])['traffic_total'].transform('std')\n",
    "\n",
    "VDS_hourly['rel_diff_avg_speed'] = (VDS_hourly['avg_speed'] - VDS_hourly['mean_avg_speed']) / VDS_hourly['mean_avg_speed']\n",
    "VDS_hourly['rel_diff_traffic']   = (VDS_hourly['traffic_total'] - VDS_hourly['mean_traffic']) / VDS_hourly['mean_traffic']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w4Pz7AdLWQk3"
   },
   "outputs": [],
   "source": [
    "VDS_hourly = u.add_lag_features_vds(VDS_hourly)\n",
    "VDS_hourly = VDS_hourly.drop(['hour', 'weekday', 'year', 'month', 'quarter'], axis=1)\n",
    "\n",
    "VDS_hourly.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add lag features for uber traveltimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_ttime_daily = u.add_lag_features_uber(segment_ttime_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_ttime_daily.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_ttime_daily['weekday'] = segment_ttime_daily['date'].astype('datetime64[ns]').dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_ttime_daily['mean_average_ttime'] = segment_ttime_daily.groupby(['segment_id', 'weekday'])['average_ttime'].transform('mean')\n",
    "segment_ttime_daily['std_average_ttime'] = segment_ttime_daily.groupby(['segment_id', 'weekday'])['average_ttime'].transform('std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_ttime_daily.drop('weekday', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dfRf7SvEHV1"
   },
   "source": [
    "## Sun altitude and azimuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J68mbsd-EOz9"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "per = pd.date_range('2016-01-01', '2019-03-31 23:00:00', freq=\"1h\")\n",
    "sun_pos = pd.DataFrame({'datetime':per})\n",
    "\n",
    "sun_pos['sun_alt'] = sun_pos['datetime'].apply(u.get_alt)\n",
    "sun_pos['sun_az']  = sun_pos['datetime'].apply(u.get_az)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mcVQu4uC9608"
   },
   "outputs": [],
   "source": [
    "train = train.loc[train['segment_id'].isin(test['segment_id'].tolist())]\n",
    "\n",
    "train = u.add_time_features(train)\n",
    "test  = u.add_time_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean accident coordinates for each segment\n",
    "coords = train_raw.groupby('road_segment_id').mean()[['longitude', 'latitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tozoDrLx960_"
   },
   "outputs": [],
   "source": [
    "weather['datetime']  = weather['datetime'].astype('datetime64[ns]')\n",
    "\n",
    "def merge_dataframes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.merge(coords, left_on='segment_id', right_on='road_segment_id', how='left')\n",
    "    df[['longitude', 'latitude']] = df[['longitude', 'latitude']].round(4)\n",
    "    df = df.merge(road_segments, on='segment_id', how='left')\n",
    "    df = df.merge(weather, on='datetime', how='left')\n",
    "    df = df.merge(sun_pos, on='datetime', how='left')\n",
    "    df = df.merge(VDS_hourly, on=['datetime', 'vds_id'], how='left')\n",
    "    df = df.merge(segment_ttime_daily, on=['segment_id','date'], how='left')\n",
    "    return df\n",
    "\n",
    "train = merge_dataframes(train)\n",
    "test  = merge_dataframes(test)\n",
    "\n",
    "train = d.optimize(train)\n",
    "test  = d.optimize(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating neighbor features for traveltime, speed and traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fval in ['traffic_total', 'avg_speed', 'average_ttime']:\n",
    "    u.add_feature_based_on_neighbs(data=train, fval=fval, num_to_neighbors=num_to_neighbors)\n",
    "    u.add_feature_based_on_neighbs(data=test, fval=fval, num_to_neighbors=num_to_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C5RPtDQei_Rg"
   },
   "outputs": [],
   "source": [
    "#Define day_period according to uber\n",
    "b = [0,7,10,16,19,24]\n",
    "l = ['AM', 'Early Morning','Midday','PM','Evening']\n",
    "train['day_period'] = pd.cut(train['hour'], bins=b, labels=l, right=False, include_lowest=True)\n",
    "test['day_period']  = pd.cut(train['hour'], bins=b, labels=l, right=False, include_lowest=True)\n",
    "\n",
    "#Calculate angle between sun azimuth and road direction\n",
    "train['blinding'] = abs(train['orientation'] - train['sun_az'])\n",
    "test['blinding']  = abs(test['orientation'] - test['sun_az'])\n",
    "\n",
    "#Calculate angle between wind and road directions\n",
    "train['angle_wind_road'] = abs(train['orientation'] - train['wind_dir_angle'])\n",
    "test['angle_wind_road']  = abs(test['orientation'] - test['wind_dir_angle'])\n",
    "\n",
    "#Calculate distance from centroid to average accident location\n",
    "train['dist_to_centroid'] = np.sqrt((train['lon_centroid'] - train['longitude']) ** 2 \n",
    "                                    + (train['lat_centroid'] - train['latitude']) ** 2)\n",
    "\n",
    "test['dist_to_centroid'] = np.sqrt((test['lon_centroid'] - test['longitude']) ** 2 \n",
    "                                    + (test['lat_centroid'] - test['latitude']) ** 2)\n",
    "\n",
    "train = u.add_public_holidays(train)\n",
    "test  = u.add_public_holidays(test)\n",
    "    \n",
    "train = u.add_school_holidays(train)\n",
    "test = u.add_school_holidays(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "99Y4apNcltD5"
   },
   "source": [
    "## Deleting cameras counts from records prior to commission_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L2KhStoRHQ5i"
   },
   "outputs": [],
   "source": [
    "commission_dates.groupby(['Commissioning Date','cls_segment_id'])['Asset Type'].agg('value_counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e4NANGxKH_2f"
   },
   "outputs": [],
   "source": [
    "train['vds_count'].loc[(train['segment_id'].isin(['5093ZHY','UY59I9M'])) & (train['datetime'] < '2016-02-05 07:00:00')] -= 1\n",
    "train['camera_count'].loc[(train['segment_id'].isin(['EUXS47I','Q2V2552'])) & (train['datetime'] < '2016-07-05 12:00:00')] -= 1\n",
    "train['camera_count'].loc[(train['segment_id'].isin(['YF1XDKK'])) & (train['datetime'] < '2016-07-05 12:00:00')] -= 2\n",
    "train['camera_count'].loc[(train['segment_id'].isin(['812PNMZ'])) & (train['datetime'] < '2016-07-05 12:00:00')] -= 3\n",
    "train['vds_count'].loc[(train['segment_id'].isin(['7QBQK9L','812PNMZ','F14EJLW'])) & (train['datetime'] < '2016-07-11 09:00:00')] -= 1\n",
    "train['camera_count'].loc[(train['segment_id'].isin(['YF1XDKK'])) & (train['datetime'] < '2016-07-19 10:00:00')] -= 1\n",
    "train['camera_count'].loc[(train['segment_id'].isin(['2M1UBKJ'])) & (train['datetime'] < '2016-12-08 08:00:00')] -= 1\n",
    "train['camera_count'].loc[(train['segment_id'].isin(['X4UA382'])) & (train['datetime'] < '2016-12-08 09:00:00')] -= 1\n",
    "train['camera_count'].loc[(train['segment_id'].isin(['N90YL69','OVJNYB2'])) & (train['datetime'] < '2017-02-02 07:00:00')] -= 1\n",
    "train['vds_count'].loc[(train['segment_id'].isin(['RZK0UM0'])) & (train['datetime'] < '2017-02-15 11:00:00')] -= 1\n",
    "train['camera_count'].loc[(train['segment_id'].isin(['N90YL69'])) & (train['datetime'] < '2017-03-20 07:00:00')] -= 1\n",
    "train['camera_count'].loc[(train['segment_id'].isin(['B9V4IAJ'])) & (train['datetime'] < '2017-05-11 07:00:00')] -= 1\n",
    "train['camera_count'].loc[(train['segment_id'].isin(['H9QJECU'])) & (train['datetime'] < '2017-10-24 10:00:00')] -= 1\n",
    "train['camera_count'].loc[(train['segment_id'].isin(['MJ9YER2'])) & (train['datetime'] < '2017-11-04 07:00:00')] -= 1\n",
    "train['vds_count'].loc[(train['segment_id'].isin(['8ITYQ2Z'])) & (train['datetime'] < '2017-11-07 07:00:00')] -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IImcBLfodq-N"
   },
   "source": [
    "## Removing silent periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "39nNHVmSeWDB",
    "outputId": "20537206-3222-4ffd-cc92-aba7ade084c9"
   },
   "outputs": [],
   "source": [
    "train = u.proc_silent_intervals(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AAkKix3q961J"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding last quarter count by sid-weekday-hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_an504HBoHSn"
   },
   "outputs": [],
   "source": [
    "train['last_quarter'] = train['datetime'].dt.year + ((train['month'] - 1) // 3) * 0.25\n",
    "test['last_quarter']  = test['datetime'].dt.year + ((test['month'] - 1) // 3) * 0.25\n",
    "\n",
    "acc_cnt_quarter_sid_wdh = pd.DataFrame(train.groupby(['segment_id', 'last_quarter', 'weekday', 'hour'])['y'].sum()) \\\n",
    "                        .reset_index().rename(columns={'y': 'acc_cnt_last_quarter_sid_wd_h'})\n",
    "\n",
    "acc_cnt_quarter_sid_wd = pd.DataFrame(train.groupby(['segment_id', 'last_quarter', 'weekday'])['y'].sum()) \\\n",
    "                        .reset_index().rename(columns={'y': 'acc_cnt_last_quarter_sid_wd'})\n",
    "\n",
    "acc_cnt_quarter_sid_h = pd.DataFrame(train.groupby(['segment_id', 'last_quarter', 'hour'])['y'].sum()) \\\n",
    "                        .reset_index().rename(columns={'y': 'acc_cnt_last_quarter_sid_h'})\n",
    "\n",
    "acc_cnt_quarter_sid = pd.DataFrame(train.groupby(['segment_id', 'last_quarter'])['y'].sum()) \\\n",
    "                        .reset_index().rename(columns={'y': 'acc_cnt_last_quarter_sid'})\n",
    "\n",
    "\n",
    "acc_cnt_quarter_vds_wdh = pd.DataFrame(train.groupby(['vds_id', 'last_quarter', 'weekday', 'hour'])['y'].sum()) \\\n",
    "                        .reset_index().rename(columns={'y': 'acc_cnt_last_quarter_vds_wd_h'})\n",
    "\n",
    "acc_cnt_quarter_vds_wd = pd.DataFrame(train.groupby(['vds_id', 'last_quarter', 'weekday'])['y'].sum()) \\\n",
    "                        .reset_index().rename(columns={'y': 'acc_cnt_last_quarter_vds_wd'})\n",
    "\n",
    "acc_cnt_quarter_vds_h = pd.DataFrame(train.groupby(['vds_id', 'last_quarter', 'hour'])['y'].sum()) \\\n",
    "                        .reset_index().rename(columns={'y': 'acc_cnt_last_quarter_vds_h'})\n",
    "\n",
    "acc_cnt_quarter_vds = pd.DataFrame(train.groupby(['vds_id', 'last_quarter'])['y'].sum()) \\\n",
    "                        .reset_index().rename(columns={'y': 'acc_cnt_last_quarter_vds'})\n",
    "\n",
    "\n",
    "\n",
    "def add_last_quarter_counts(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.merge(acc_cnt_quarter_sid_wdh, on=['segment_id', 'last_quarter', 'weekday', 'hour'], how='left')\n",
    "    df = df.merge(acc_cnt_quarter_sid_wd, on=['segment_id', 'last_quarter', 'weekday'], how='left')\n",
    "    df = df.merge(acc_cnt_quarter_sid_h, on=['segment_id', 'last_quarter', 'hour'], how='left')\n",
    "    df = df.merge(acc_cnt_quarter_sid, on=['segment_id', 'last_quarter'], how='left')\n",
    "    df = df.merge(acc_cnt_quarter_vds_wdh, on=['vds_id', 'last_quarter', 'weekday', 'hour'], how='left')\n",
    "    df = df.merge(acc_cnt_quarter_vds_wd, on=['vds_id', 'last_quarter', 'weekday'], how='left')\n",
    "    df = df.merge(acc_cnt_quarter_vds_h, on=['vds_id', 'last_quarter', 'hour'], how='left')\n",
    "    df = df.merge(acc_cnt_quarter_vds, on=['vds_id', 'last_quarter'], how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "train['last_quarter'] -= 0.25\n",
    "test['last_quarter']  -= 0.25\n",
    "\n",
    "train = add_last_quarter_counts(train)\n",
    "test  = add_last_quarter_counts(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = d.optimize(train)\n",
    "test = d.optimize(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding last halfyear count by sid-weekday-hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['last_halfyear'] = train['datetime'].dt.year + ((train['month'] - 1) // 6) * 0.5\n",
    "test['last_halfyear']  = test['datetime'].dt.year + ((test['month'] - 1) // 6) * 0.5\n",
    "\n",
    "acc_cnt_halfyear_sid_wdh = pd.DataFrame(train.groupby(['segment_id', 'last_halfyear', 'weekday', 'hour'])['y'].sum()) \\\n",
    "                        .reset_index().rename(columns={'y': 'acc_cnt_last_halfyear_sid_wd_h'})\n",
    "\n",
    "acc_cnt_halfyear_sid_wd = pd.DataFrame(train.groupby(['segment_id', 'last_halfyear', 'weekday'])['y'].sum()) \\\n",
    "                        .reset_index().rename(columns={'y': 'acc_cnt_last_halfyear_sid_wd'})\n",
    "\n",
    "acc_cnt_halfyear_sid_h = pd.DataFrame(train.groupby(['segment_id', 'last_halfyear', 'hour'])['y'].sum()) \\\n",
    "                        .reset_index().rename(columns={'y': 'acc_cnt_last_halfyear_sid_h'})\n",
    "\n",
    "acc_cnt_halfyear_sid = pd.DataFrame(train.groupby(['segment_id', 'last_halfyear'])['y'].sum()) \\\n",
    "                        .reset_index().rename(columns={'y': 'acc_cnt_last_halfyear_sid'})\n",
    "\n",
    "\n",
    "acc_cnt_halfyear_vds_wdh = pd.DataFrame(train.groupby(['vds_id', 'last_halfyear', 'weekday', 'hour'])['y'].sum()) \\\n",
    "                        .reset_index().rename(columns={'y': 'acc_cnt_last_halfyear_vds_wd_h'})\n",
    "\n",
    "acc_cnt_halfyear_vds_wd = pd.DataFrame(train.groupby(['vds_id', 'last_halfyear', 'weekday'])['y'].sum()) \\\n",
    "                        .reset_index().rename(columns={'y': 'acc_cnt_last_halfyear_vds_wd'})\n",
    "\n",
    "acc_cnt_halfyear_vds_h = pd.DataFrame(train.groupby(['vds_id', 'last_halfyear', 'hour'])['y'].sum()) \\\n",
    "                        .reset_index().rename(columns={'y': 'acc_cnt_last_halfyear_vds_h'})\n",
    "\n",
    "acc_cnt_halfyear_vds = pd.DataFrame(train.groupby(['vds_id', 'last_halfyear'])['y'].sum()) \\\n",
    "                        .reset_index().rename(columns={'y': 'acc_cnt_last_halfyear_vds'})\n",
    "\n",
    "\n",
    "\n",
    "def add_last_halfyear_counts(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.merge(acc_cnt_halfyear_sid_wdh, on=['segment_id', 'last_halfyear', 'weekday', 'hour'], how='left')\n",
    "    df = df.merge(acc_cnt_halfyear_sid_wd, on=['segment_id', 'last_halfyear', 'weekday'], how='left')\n",
    "    df = df.merge(acc_cnt_halfyear_sid_h, on=['segment_id', 'last_halfyear', 'hour'], how='left')\n",
    "    df = df.merge(acc_cnt_halfyear_sid, on=['segment_id', 'last_halfyear'], how='left')\n",
    "    df = df.merge(acc_cnt_halfyear_vds_wdh, on=['vds_id', 'last_halfyear', 'weekday', 'hour'], how='left')\n",
    "    df = df.merge(acc_cnt_halfyear_vds_wd, on=['vds_id', 'last_halfyear', 'weekday'], how='left')\n",
    "    df = df.merge(acc_cnt_halfyear_vds_h, on=['vds_id', 'last_halfyear', 'hour'], how='left')\n",
    "    df = df.merge(acc_cnt_halfyear_vds, on=['vds_id', 'last_halfyear'], how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "train['last_halfyear'] -= 0.5\n",
    "test['last_halfyear']  -= 0.5\n",
    "\n",
    "train = add_last_halfyear_counts(train)\n",
    "test  = add_last_halfyear_counts(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CYmh4y6sebd2"
   },
   "outputs": [],
   "source": [
    "train = d.optimize(train)\n",
    "test = d.optimize(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding neighbor features for sids and last quarter/ halfyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fval in ['acc_cnt_last_quarter_sid_wd_h', 'acc_cnt_last_quarter_sid_wd',\n",
    "            'acc_cnt_last_quarter_sid_h','acc_cnt_last_quarter_sid',\n",
    "             \n",
    "            'acc_cnt_last_halfyear_sid_wd_h', 'acc_cnt_last_halfyear_sid_wd',\n",
    "            'acc_cnt_last_halfyear_sid_h', 'acc_cnt_last_halfyear_sid']:\n",
    "    \n",
    "    add_feature_based_on_neighbs(data=train, fval=fval, num_to_neighbors=num_to_neighbors)\n",
    "    add_feature_based_on_neighbs(data=test, fval=fval, num_to_neighbors=num_to_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = d.optimize(train)\n",
    "test = d.optimize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5IlLKvy5J-we"
   },
   "outputs": [],
   "source": [
    "train.drop(['date', 'num', 'last_quarter', 'last_halfyear'], axis=1).to_pickle('../data/train_1002.pkl')\n",
    "test.drop(['date', 'num', 'last_quarter', 'last_halfyear'], axis=1).to_pickle('../data/test_1002.pkl')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "zindi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
